#!/usr/bin/env python3

import os 
import sys 
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from xauto.bootstrap.build import bootstrap
if not bootstrap():
    print("Bootstrap failed")
    sys.exit(1)

from xauto.utils.config import Config
from xauto.utils.page_loading import wait_for_page_load
from xauto.utils.validation import is_bot_page
from xauto.runtime.lifecycle import setup_runtime, teardown_runtime

import lxml.html
from urllib.parse import urljoin

config = Config()

# you can edit config settings after the inital settings.yaml load
config.set("proxy.enabled", False)
config.set("driver.headless", True)

# freeze confing once you no longer need to change settings
config.freeze()

TARGETS = [
    # {'url': 'https://www.example.com/', 'source': 'example'},
    {'url': 'https://www.exploit-db.com/', 'source': 'exploitdb'},
]

RAW_COMMENTS = ('#', '//', ';')

def parse_exploitdb(driver, limit=5):
    print("[xauto] Starting parse_exploitdb")

    # explicitly wait for X amount of time
    # if not explicit_page_load(driver, wait_for=2):
    #     print("Page did not load")
    #     return

    if not wait_for_page_load(driver, timeout=2):
        print("Page did not load")
        return

    if is_bot_page(driver, driver.current_url):
        print("Bot page has been detected")
        return

    root = lxml.html.fromstring(driver.page_source)
    rows = root.xpath('//table[@id="exploits-table"]//tbody/tr')[:limit]
    print(f"[xauto] Found {len(rows)} rows (limited to {limit})")

    entries = []
    for i, row in enumerate(rows):
        print(f"[xauto] Processing row {i}")
        tds = row.xpath('./td')
        if len(tds) < 7:
            continue

        tl = tds[4].xpath('.//a')
        if not tl:
            continue

        entries.append({
            'date': tds[0].text_content().strip(),
            'title': tl[0].text_content().strip(),
            'type': tds[5].text_content().strip(),
            'platform': tds[6].text_content().strip(),
            'detail_url': tl[0].get('href'),
            'exploit_id': tl[0].get('href').strip('/').split('/')[-1],
        })

    return entries

def scrape(driver, detail_url):
    print(f"\n[xauto] Visiting detail page: {detail_url}")
    base_url = TARGETS[0]['url']
    driver.get(urljoin(base_url, detail_url))

    if not wait_for_page_load(driver):
        print("Page did not load")
        return

    exploit_id = detail_url.strip("/").split("/")[-1]
    raw_url = f"{base_url}raw/{exploit_id}"
    print(f"[xauto] Visiting raw page: {raw_url}")
    driver.get(raw_url)

    if not wait_for_page_load(driver):
        print("Page did not load")
        return

    raw_dom = lxml.html.fromstring(driver.page_source)
    pre = raw_dom.xpath('//pre/text()')
    code = pre[0] if pre else ""
    comments = []

    for line in code.splitlines():
        striped = line.strip()
        if striped.startswith(RAW_COMMENTS):
            comments.append(striped)
        elif striped:
            break

    return {
        'exploit_id': exploit_id,
        'raw_url': raw_url,
        'comments': comments,
        'raw_code': code,
    }

def get_data(task, driver):
    driver.get(task['url'])
    
    if not wait_for_page_load(driver):
        print("Page did not load")
        return
    
    if is_bot_page(driver, driver.current_url):
        print("Bot page has been detected")
        return

    entries = parse_exploitdb(driver)
    if not entries:
        return
    
    out_file = "exploitdb_results.md"
    with open(out_file, "w", encoding="utf-8") as f:
        for e in entries:
            comments_data = {}
            if e['exploit_id']:
                comments_data = scrape(driver, e['detail_url'])
                if not comments_data:
                    return

            print(f"[xauto] Visiting exploit ID {e['exploit_id']}")

            f.write(f"> Exploit ID: {e['exploit_id']}\n")
            f.write(f"Date: {e.get('date', 'N/A')}\n")
            f.write(f"Title: {e.get('title', 'N/A')}\n")
            f.write(f"Platform: {e.get('platform', 'N/A')}\n")
            f.write(f"Type: {e.get('type', 'N/A')}\n")
            f.write(f"Raw URL: {comments_data.get('raw_url', 'N/A')}\n")
            f.write(f"Comments:\n")
            for line in comments_data.get('comments', []):
                f.write(f"    {line}\n")
            f.write("\n")  

    print(f"[xauto] Data saved to {out_file}")

task_manager, driver_pool = setup_runtime(
    task_processor=get_data,
)

for task in TARGETS:
    task_manager.add_task(task)

task_manager.wait_completion()
teardown_runtime(task_manager, driver_pool)
